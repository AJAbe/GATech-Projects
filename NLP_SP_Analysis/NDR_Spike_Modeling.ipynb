{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26040ff5",
   "metadata": {},
   "source": [
    "# NDR Spike Modelling\n",
    "##### This notebook contains details about the different Spike related models created based on each of the 3 Sections (1,1A and 7).\n",
    "##### The input to this notebook are the 3 pickle files generated from the prior PreProcessing step\n",
    "##### Most of the models here are quick and should execute within 5-7 minutes, but the one utilizing Word2Vec (Spike based on Word2Vec and Global Word2Vec) could take > 4-5 hours depending on processing speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9fb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re as re\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Phrases\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from scipy.ndimage import maximum_filter1d\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()# Visualise inside a notebook\n",
    "#import en_core_web_md\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaMulticore, LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from gensim import corpora\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def peaks_previousN(a, n):\n",
    "    W = (n-1)//2\n",
    "    return np.flatnonzero(a[1:]>maximum_filter1d(a, n, origin=W)[:-1])+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d00b4",
   "metadata": {},
   "source": [
    "##### This might be redundant since we took care of stop word removal as part of pre-processing but we still use stop word removal as part of running as NLP vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "stops_list =['table_end','table_start','president','vice','business','chief','exerience','mhz','usa','senior','www','world',\\\n",
    "'january','february','march',\\\n",
    "'april','may','june','july','august','september','october','november','december','year','mr','north','america','industry',\\\n",
    "'payment','solution','solutions','i','ii','iii','iv','v','vi','vii','viii','company','product','management','customer',\\\n",
    "'item','ability','access','annual','report','broad','related','system','service','uncertain','unauthorized','false',\\\n",
    "'system','uk','revenue','earning','approval','region','example','incident','company','financial','statements','supplementary',\\\n",
    "'data','summary','notes','consolidated','note','notes','due', 'adjusted','review','versus','herein','reference','industrial', \\\n",
    "             'products','overall','result','aa','aaa','write','years','year','able','acceptable','acceptance'\\\n",
    "            ]\n",
    "stops=stops+stops_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a0f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_data=pd.read_csv(\"industry.txt\", delimiter=\"|\")\n",
    "industry_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a1b78",
   "metadata": {},
   "source": [
    "##### The below analysis are all based on top 75 industries from Company count, but it could adjusted accordingly as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab63d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_data.dropna(subset=['ID','SubInd'], inplace=True)\n",
    "industry_data['ID']=industry_data['ID'].astype('int')\n",
    "main_75_industry = list(industry_data['SubInd'].value_counts()[:75].index)\n",
    "#main_75_industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_75_industry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a63302",
   "metadata": {},
   "source": [
    "##### Here we start reading individual preprocessed pickle files from the earlier notebook and start executing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec4e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge_1 = pd.read_pickle(\"item1_cleaned.pkl\")  \n",
    "final_merge_1=final_merge_1[['date','cik','company','year','item1_cleaned']]\n",
    "final_merge_1.rename(columns={'year':'actual_year', 'company': 'actual_company', 'date': 'actual_date'},inplace=True)\n",
    "final_merge_1.dropna(subset=['actual_year', 'item1_cleaned'], inplace=True)\n",
    "final_merge_1.reset_index(drop=True,inplace=True)\n",
    "final_merge_1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d1de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da66dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge_1 = final_merge_1.merge(industry_data[['ID', 'SubInd']], left_on = 'cik', right_on='ID',how = 'left')\n",
    "final_merge_1.drop(['ID'], axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a574025",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge_1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_new_nlp = TfidfVectorizer(ngram_range = (1,3), max_features = 3000,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "#                       stop_words = stops, max_df=.75) \n",
    "# dtm_new_nlp = vec_new_nlp.fit_transform(final_merge_1['item1_cleaned'])\n",
    "\n",
    "# dtm_tfidf_new= pd.DataFrame(dtm_new_nlp.toarray())\n",
    "# d = dict(zip(list(dtm_tfidf_new.columns), list(np.array(vec_new_nlp.get_feature_names_out()))))\n",
    "# dtm_tfidf_new.rename(columns=d, inplace=True)\n",
    "# final_merge_sec1 = pd.concat([final_merge_1, dtm_tfidf_new], axis=1)\n",
    "# final_merge_sec1['cik']=final_merge_sec1['cik'].astype('object')\n",
    "# final_merge_sec1['actual_year']=final_merge_sec1['actual_year'].astype('object')\n",
    "\n",
    "# numeric_cols = list(final_merge_sec1.select_dtypes(include='number').columns)\n",
    "# numeric_cols\n",
    "\n",
    "# final_merge_transpose = final_merge_sec1.groupby(['actual_year'])[numeric_cols].sum()\n",
    "\n",
    "# final_merge_groupby_subind = final_merge_sec1.groupby(['SubInd','actual_year'])[numeric_cols].sum().reset_index()\n",
    "# final_merge_groupby_subind.head(5) #here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702fc5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56797e34",
   "metadata": {},
   "source": [
    "### Sec 1 - Model 1: \n",
    "##### The below code would create a dictionary object that has the top words / bigrams / trigrams based on Section 1, of the SEC records belonging to the top 75 industries split by year. The top word identification is done by using a custom function that finds peaks in data based on its tfidf distribution\n",
    "##### Configurations that can be modified are highlighted as comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619c4d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec1_tfidf_model_1=defaultdict(defaultdict(list).copy)\n",
    "\n",
    "try:\n",
    "    \n",
    "    for item in main_75_industry:\n",
    "        if(final_merge_1[final_merge_1['SubInd']==item].shape[0]==0): continue\n",
    "        new_df=final_merge_1[final_merge_1['SubInd']==item].reset_index()\n",
    "        new_df=new_df.drop(['SubInd'], axis=1)\n",
    "\n",
    "        vec_new_nlp = TfidfVectorizer(ngram_range = (1,3), max_features = 3000,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "                          stop_words = stops,  max_df=.7) # ngram_range, max_features, max_df, min_df can be adjusted\n",
    "        dtm_new_nlp = vec_new_nlp.fit_transform(new_df['item1_cleaned'])\n",
    "\n",
    "\n",
    "        dtm_tfidf_new= pd.DataFrame(dtm_new_nlp.toarray())\n",
    "        d = dict(zip(list(dtm_tfidf_new.columns), list(np.array(vec_new_nlp.get_feature_names_out()))))\n",
    "        dtm_tfidf_new.rename(columns=d, inplace=True)\n",
    "        final_merge_sec1 = pd.concat([new_df, dtm_tfidf_new], axis=1)\n",
    "        final_merge_sec1['cik']=final_merge_sec1['cik'].astype('object')\n",
    "        final_merge_sec1['actual_year']=final_merge_sec1['actual_year'].astype('object')\n",
    "\n",
    "        numeric_cols = list(final_merge_sec1.select_dtypes(include='number').columns)\n",
    "        #numeric_cols\n",
    "\n",
    "        new_df_next=final_merge_sec1.groupby(['actual_year'])[numeric_cols].sum()\n",
    "        b_next=new_df_next.transpose()\n",
    "        b_next=b_next[(b_next.max(axis=1) >= .25) ] # This signifies that the word should be present with a \n",
    "        #higher relavance in some of the years. The number can be adjusted\n",
    "        new_df_next=b_next.transpose()\n",
    "        new_df_next\n",
    "\n",
    "        for item_2 in list(new_df_next.columns):\n",
    "            if item_2=='index': continue\n",
    "            testing_peaks=new_df_next.loc[:,item_2]\n",
    "            peaks=peaks_previousN(testing_peaks.values, 22) # This is the custom function that generates the spike words. \n",
    "\n",
    "            for next_item in list(testing_peaks.iloc[peaks].index):\n",
    "                if (item not in sec1_tfidf_model_1.keys()): sec1_tfidf_model_1[item]=defaultdict(list)\n",
    "                sec1_tfidf_model_1[item][next_item].append(item_2)\n",
    "\n",
    "        if item in sec1_tfidf_model_1.keys(): sec1_tfidf_model_1[item]=dict(sorted(sec1_tfidf_model_1[item].items()))\n",
    "\n",
    "            #iter_dict[item].append(list(testing_peaks.iloc[peaks].index))\n",
    "        sec1_tfidf_model_1=dict(sorted(sec1_tfidf_model_1.items()))\n",
    "        \n",
    "        # The final dict object is in sec1_tfidf_model_1\n",
    "\n",
    "except:\n",
    "    print(item)\n",
    "    print(peaks)\n",
    "    #print(list(new_df_next.columns))\n",
    "    #print(list(testing_peaks.iloc[peaks]))\n",
    "    #print(list(testing_peaks.iloc[peaks].index))\n",
    "    #print(testing_peaks)\n",
    "#sec1_tfidf_model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22513923",
   "metadata": {},
   "source": [
    "### If we need to visualize this function, execute the below two sections. As an example, this is to see the covid-19 word that was captured from the model within the Biotech industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=final_merge_1[final_merge_1['SubInd']=='Biotechnology'].reset_index()\n",
    "new_df=new_df.drop(['SubInd'], axis=1)\n",
    "\n",
    "vec_new_nlp = TfidfVectorizer(ngram_range = (1,3), max_features = 3000,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "                  stop_words = stops,  max_df=.7) \n",
    "\n",
    "dtm_new_nlp = vec_new_nlp.fit_transform(new_df['item1_cleaned'])\n",
    "\n",
    "\n",
    "dtm_tfidf_new= pd.DataFrame(dtm_new_nlp.toarray())\n",
    "d = dict(zip(list(dtm_tfidf_new.columns), list(np.array(vec_new_nlp.get_feature_names_out()))))\n",
    "dtm_tfidf_new.rename(columns=d, inplace=True)\n",
    "final_merge_sec1 = pd.concat([new_df, dtm_tfidf_new], axis=1)\n",
    "final_merge_sec1['cik']=final_merge_sec1['cik'].astype('object')\n",
    "final_merge_sec1['actual_year']=final_merge_sec1['actual_year'].astype('object')\n",
    "\n",
    "numeric_cols = list(final_merge_sec1.select_dtypes(include='number').columns)\n",
    "#numeric_cols\n",
    "\n",
    "new_df_next=final_merge_sec1.groupby(['actual_year'])[numeric_cols].sum()\n",
    "b_next=new_df_next.transpose()\n",
    "b_next=b_next[(b_next.max(axis=1) >= .25) ] \n",
    "new_df_next=b_next.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_peaks=new_df_next.loc[:,'covid-19']\n",
    "peaks=peaks_previousN(testing_peaks.values, 22)\n",
    "\n",
    "plt.plot(testing_peaks)\n",
    "plt.plot(testing_peaks.iloc[peaks].index, testing_peaks.iloc[peaks].values, \"x\")\n",
    "#plt.plot(np.zeros_like(testing_peaks), \"--\", color=\"gray\")\n",
    "plt.title(\"Spikes for Covid-19 across years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fead7a",
   "metadata": {},
   "source": [
    "### Section 1 - Model 2: \n",
    "##### The below code would create a dictionary object that has the top words / bigrams / trigrams based on Section 1, in the SEC records belonging to the top 75 industries split by year. The top word identification is done by using a custom function that finds peaks based on how many sigmas away the peak values are from the norm.\n",
    "##### Configurations that can be modified are highlighted as comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f700bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec1_tfidf_model_2=defaultdict(defaultdict(list).copy)\n",
    "\n",
    "try:\n",
    "    \n",
    "    for item in main_75_industry:\n",
    "        if(final_merge_1[final_merge_1['SubInd']==item].shape[0]==0): continue\n",
    "        new_df=final_merge_1[final_merge_1['SubInd']==item].reset_index()\n",
    "        new_df=new_df.drop(['SubInd'], axis=1)\n",
    "\n",
    "        vec_new_nlp = TfidfVectorizer(ngram_range = (1,3), max_features = 3000,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "                          stop_words = stops,  max_df=.75)  # ngram_range, max_features, max_df, min_df can be adjusted\n",
    "        # re can be adjusted also\n",
    "        dtm_new_nlp = vec_new_nlp.fit_transform(new_df['item1_cleaned'])\n",
    "\n",
    "\n",
    "        dtm_tfidf_new= pd.DataFrame(dtm_new_nlp.toarray())\n",
    "        d = dict(zip(list(dtm_tfidf_new.columns), list(np.array(vec_new_nlp.get_feature_names_out()))))\n",
    "        dtm_tfidf_new.rename(columns=d, inplace=True)\n",
    "        final_merge_sec1 = pd.concat([new_df, dtm_tfidf_new], axis=1)\n",
    "        final_merge_sec1['cik']=final_merge_sec1['cik'].astype('object')\n",
    "        final_merge_sec1['actual_year']=final_merge_sec1['actual_year'].astype('object')\n",
    "\n",
    "        numeric_cols = list(final_merge_sec1.select_dtypes(include='number').columns)\n",
    "        #numeric_cols\n",
    "\n",
    "        new_df_next=final_merge_sec1.groupby(['actual_year'])[numeric_cols].sum()\n",
    "        b_next=new_df_next.transpose()\n",
    "        b_next=b_next[(b_next.max(axis=1) >= .15)  ] # This signifies that the word should be present with a \n",
    "        # higher relavance in some of the years\n",
    "        new_df_next=b_next.transpose()\n",
    "        new_df_next\n",
    "\n",
    "        for item_2 in list(new_df_next.columns):\n",
    "            if item_2=='index': continue\n",
    "            testing_peaks=new_df_next.loc[:,item_2]\n",
    "            \n",
    "            \n",
    "            \n",
    "            peaks=np.argwhere((testing_peaks-np.mean(testing_peaks))/np.std(testing_peaks) > 2).ravel() \n",
    "            # Here, we look at peaks that occur more than 2 Sigmas away\n",
    "            \n",
    "\n",
    "            for next_item in list(testing_peaks.iloc[peaks].index):\n",
    "                if (item not in sec1_tfidf_model_2.keys()): sec1_tfidf_model_2[item]=defaultdict(list)\n",
    "                sec1_tfidf_model_2[item][next_item].append(item_2)\n",
    "\n",
    "        if item in sec1_tfidf_model_2.keys(): sec1_tfidf_model_2[item]=dict(sorted(sec1_tfidf_model_2[item].items()))\n",
    "\n",
    "            #iter_dict[item].append(list(testing_peaks.iloc[peaks].index))\n",
    "        sec1_tfidf_model_2=dict(sorted(sec1_tfidf_model_2.items()))\n",
    "        \n",
    "        # The dict model would be available in sec1_tfidf_model_2\n",
    "\n",
    "except:\n",
    "    print(item)\n",
    "    print(peaks)\n",
    "    #print(list(new_df_next.columns))\n",
    "    #print(list(testing_peaks.iloc[peaks]))\n",
    "    #print(list(testing_peaks.iloc[peaks].index))\n",
    "    #print(testing_peaks)\n",
    "#sec1_tfidf_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b9c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict(sorted(sec1_tfidf_model_2.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a55241",
   "metadata": {},
   "source": [
    "### Sec 1 - Model 3: The below code would create a dictionary object that has the top words / bigrams / trigrams based on Section 1, in the SEC records belonging to the top 75 industries split by year. The top word identification is done taking the top 20 words per Industry per year\n",
    "##### Configurations that can be modified are highlighted as comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941e17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec1_tfidf_model_3=defaultdict(defaultdict(list).copy)\n",
    "\n",
    "\n",
    "    \n",
    "for item in main_75_industry:\n",
    "    if(final_merge_1[final_merge_1['SubInd']==item].shape[0]==0): continue\n",
    "    new_df=final_merge_1[final_merge_1['SubInd']==item].reset_index()\n",
    "    new_df=new_df.drop(['SubInd','index'], axis=1)\n",
    "\n",
    "    vec_new_nlp = TfidfVectorizer(ngram_range = (1,3), max_features = 1000,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "                      stop_words = stops,  max_df=.75) \n",
    "    # ngram_range, max_features, max_df, min_df can be adjusted\n",
    "        # re can be adjusted also\n",
    "        \n",
    "    dtm_new_nlp = vec_new_nlp.fit_transform(new_df['item1_cleaned'])\n",
    "\n",
    "\n",
    "    dtm_tfidf_new= pd.DataFrame(dtm_new_nlp.toarray())\n",
    "    d = dict(zip(list(dtm_tfidf_new.columns), list(np.array(vec_new_nlp.get_feature_names_out()))))\n",
    "    dtm_tfidf_new.rename(columns=d, inplace=True)\n",
    "\n",
    "\n",
    "    final_merge_sec1 = pd.concat([new_df, dtm_tfidf_new], axis=1)\n",
    "    final_merge_sec1['cik']=final_merge_sec1['cik'].astype('object')\n",
    "    final_merge_sec1['actual_year']=final_merge_sec1['actual_year'].astype('object')\n",
    "\n",
    "\n",
    "\n",
    "    numeric_cols = list(final_merge_sec1.select_dtypes(include='number').columns)\n",
    "    #numeric_cols\n",
    "\n",
    "    new_df_next=final_merge_sec1.groupby(['actual_year'])[numeric_cols].sum()\n",
    "    #print(new_df_next.head(5))\n",
    "\n",
    "    for item_2 in new_df_next.index:\n",
    "        temp_df=new_df_next.loc[item_2]\n",
    "        #top20 = temp_df.values.argsort()[::-1][:20]\n",
    "        #top20=sparse_argsort(temp_df)[::-1][:20]\n",
    "        top20=np.argsort(temp_df.values)[np.in1d(np.argsort(temp_df.values),np.where(temp_df.values),1)][::-1][:20] \n",
    "        # Here we get the top 20 words\n",
    "        #print(new_df_next.head(2) )\n",
    "        #print(item)\n",
    "        #print(top20)\n",
    "        #print(temp_df.values)\n",
    "        top_20_features=[vec_new_nlp.get_feature_names_out()[i] for i in list(top20)] \n",
    "        if (item not in sec1_tfidf_model_3.keys()): sec1_tfidf_model_3[item]=defaultdict(list)\n",
    "        sec1_tfidf_model_3[item][item_2].append(top_20_features)\n",
    "\n",
    "\n",
    "\n",
    "    if item in sec1_tfidf_model_3.keys(): sec1_tfidf_model_3[item]=dict(sorted(sec1_tfidf_model_3[item].items()))\n",
    "\n",
    "        #iter_dict[item].append(list(testing_peaks.iloc[peaks].index))\n",
    "    sec1_tfidf_model_3=dict(sorted(sec1_tfidf_model_3.items()))\n",
    "\n",
    "\n",
    "#sec1_tfidf_model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab8b015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8d1305a",
   "metadata": {},
   "source": [
    "### Sec 1 - Model 4: The below code takes the top 20 words for each of the 75 Industries across all years. Then it created seperate Word2Vec model for each year and 10 most similar words to that. This is time consuming model but the output dictionary does give back valuable inights on how words trended across years. This model could be further updated to input a different set of 'Spike' words to the Word2Vec model to get the corresponding similar words; and even the Word2Vec models can be modified.\n",
    "##### Configurations that can be modified are highlighted as comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029668e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dict_ind_top20_1_similar=defaultdict(defaultdict(defaultdict(dict).copy).copy)\n",
    "\n",
    "\n",
    "    \n",
    "for item in main_75_industry:\n",
    "    if(final_merge[final_merge['SubInd']==item].shape[0]==0): continue\n",
    "    new_df=final_merge[final_merge['SubInd']==item].reset_index()\n",
    "    new_df=new_df.drop(['SubInd','index'], axis=1)\n",
    "\n",
    "    vec_new_nlp = TfidfVectorizer(ngram_range = (1,3), max_features = 1000,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "                      stop_words = stops,  max_df=.75) \n",
    "    \n",
    "        # ngram_range, max_features, max_df, min_df can be adjusted\n",
    "        # re can be adjusted also\n",
    "        \n",
    "    dtm_new_nlp = vec_new_nlp.fit_transform(new_df['item1_cleaned'])\n",
    "\n",
    "\n",
    "    dtm_tfidf_new= pd.DataFrame(dtm_new_nlp.toarray())\n",
    "    d = dict(zip(list(dtm_tfidf_new.columns), list(np.array(vec_new_nlp.get_feature_names_out()))))\n",
    "    dtm_tfidf_new.rename(columns=d, inplace=True)\n",
    "\n",
    "\n",
    "    final_merge_sec1 = pd.concat([new_df, dtm_tfidf_new], axis=1)\n",
    "    final_merge_sec1['cik']=final_merge_sec1['cik'].astype('object')\n",
    "    final_merge_sec1['actual_year']=final_merge_sec1['actual_year'].astype('object')\n",
    "\n",
    "\n",
    "\n",
    "    numeric_cols = list(final_merge_sec1.select_dtypes(include='number').columns)\n",
    "    #numeric_cols\n",
    "\n",
    "    new_df_next=final_merge_sec1.groupby(['actual_year'])[numeric_cols].sum()\n",
    "    \n",
    "    new_df_next_sum=np.sum(new_df_next, axis=0)\n",
    "    \n",
    "    top20=np.argsort(new_df_next_sum.values)[np.in1d(np.argsort(new_df_next_sum.values),\\\n",
    "                                                     np.where(new_df_next_sum.values),1)][::-1][:20]\n",
    "    top_20_features=[vec_new_nlp.get_feature_names_out()[i] for i in list(top20)] \n",
    "    \n",
    "    \n",
    "    #print(new_df_next.head(5))\n",
    "\n",
    "    for item_2 in new_df_next.index:\n",
    "        #temp_df=new_df_next.loc[item_2]\n",
    "        #top20 = temp_df.values.argsort()[::-1][:20]\n",
    "        #top20=sparse_argsort(temp_df)[::-1][:20]\n",
    "        all_sentences_inter = final_merge[final_merge['actual_year']==item_2]['item1_cleaned'].apply(lambda x: x.split())\n",
    "        \n",
    "        #Different configurations can be done on the Word2Vec model especially aroud min_count, scoring and threshold\n",
    "        \n",
    "        \n",
    "        bigram_transformer = Phrases(all_sentences_inter, min_count=20, delimiter=' ')\n",
    "        bigram = Phraser(bigram_transformer)\n",
    "        \n",
    "        trigram_txnformer=Phrases(bigram[all_sentences_inter], min_count=20, delimiter=' ')\n",
    "        trigram = Phraser(trigram_txnformer)\n",
    "        \n",
    "        w2v_model_bi_tri = Word2Vec(trigram[all_sentences_inter], min_count=20)\n",
    "        \n",
    "        for item3 in top_20_features:\n",
    "            if ' ' not in item3 and item3 in w2v_model_bi_tri.wv.key_to_index.keys() :\n",
    "                ea = w2v_model_bi_tri.wv.most_similar(item3)\n",
    "            elif all(x in w2v_model_bi_tri.wv.key_to_index.keys() for x in item3):\n",
    "                \n",
    "                ea = w2v_model_bi_tri.wv.most_similar(positive=item3.split())\n",
    "\n",
    "        \n",
    "            if (item not in iter_dict_ind_top20_1_similar.keys()): \n",
    "                iter_dict_ind_top20_1_similar[item]=defaultdict(dict)\n",
    "            if (item_2 not in  iter_dict_ind_top20_1_similar[item].keys()):\n",
    "                iter_dict_ind_top20_1_similar[item][item_2] = defaultdict(dict)\n",
    "            iter_dict_ind_top20_1_similar[item][item_2][item3] = ea[:10]\n",
    "\n",
    "\n",
    "\n",
    "    if item in iter_dict_ind_top20_1_similar.keys(): iter_dict_ind_top20_1_similar[item]=\\\n",
    "        dict(sorted(iter_dict_ind_top20_1_similar[item].items()))\n",
    "\n",
    "        #iter_dict[item].append(list(testing_peaks.iloc[peaks].index))\n",
    "    iter_dict_ind_top20_1_similar=dict(sorted(iter_dict_ind_top20_1_similar.items()))\n",
    "\n",
    "\n",
    "iter_dict_ind_top20_1_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca634319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33714251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc5e7467",
   "metadata": {},
   "source": [
    "## Loading Section 1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44fc457",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge_1A = pd.read_pickle(\"item1A_cleaned.pkl\")  \n",
    "final_merge_1A.rename(columns={'year':'actual_year', 'company': 'actual_company', 'date': 'actual_date'},inplace=True)\n",
    "final_merge_1A=final_merge_1A[['actual_date','cik','actual_company','actual_year','item1A_cleaned']]\n",
    "#final_merge_1A.rename(columns={'year':'actual_year', 'company': 'actual_company', 'date': 'actual_date'},inplace=True)\n",
    "final_merge_1A.dropna(subset=['actual_year', 'item1A_cleaned'], inplace=True)\n",
    "final_merge_1A = final_merge_1A.merge(industry_data[['ID', 'SubInd']], left_on = 'cik', right_on='ID',how = 'left')\n",
    "final_merge_1A.drop(['ID'], axis=1,inplace=True)\n",
    "final_merge_1A #11400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f795890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_new_nlp_1A = TfidfVectorizer(ngram_range = (1,3), max_features = 2500,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "#                       stop_words = stops, min_df=.15, max_df=.6) \n",
    "# dtm_new_nlp_1A = vec_new_nlp_1A.fit_transform(final_merge_1A['item1A_cleaned'])\n",
    "\n",
    "# dtm_tfidf_new_1A= pd.DataFrame(dtm_new_nlp_1A.toarray())\n",
    "# d = dict(zip(list(dtm_tfidf_new_1A.columns), list(np.array(vec_new_nlp_1A.get_feature_names_out()))))\n",
    "# dtm_tfidf_new_1A.rename(columns=d, inplace=True)\n",
    "# final_merge_sec_1A = pd.concat([final_merge_1A, dtm_tfidf_new_1A], axis=1)\n",
    "# final_merge_sec_1A['cik']=final_merge_sec_1A['cik'].astype('object')\n",
    "# final_merge_sec_1A['actual_year']=final_merge_sec_1A['actual_year'].astype('object')\n",
    "\n",
    "# numeric_cols = list(final_merge_sec_1A.select_dtypes(include='number').columns)\n",
    "# final_merge_transpose_1A = final_merge_sec_1A.groupby(['actual_year'])[numeric_cols].sum()\n",
    "\n",
    "# final_merge_groupby_subind_1A = final_merge_sec_1A.groupby(['SubInd','actual_year'])[numeric_cols].sum().reset_index()\n",
    "# final_merge_groupby_subind_1A.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c0125b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14881b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07b988e9",
   "metadata": {},
   "source": [
    "### Sec 1A - Model 1: \n",
    "##### The below code would create a dictionary object that has the top words / bigrams / trigrams based on Section 1A, in the SEC records belonging to the top 75 industries split by year. The top word identification is done by using a custom function that finds peaks in data based on its tfidf distribution\n",
    "##### Configurations that can be modified are highlighted as comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b05033",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec1A_tfidf_model_1=defaultdict(defaultdict(list).copy)\n",
    "\n",
    "try:\n",
    "    \n",
    "    for item in main_75_industry:\n",
    "        if(final_merge_1A[final_merge_1A['SubInd']==item].shape[0]==0): continue\n",
    "        new_df=final_merge_1A[final_merge_1A['SubInd']==item].reset_index()\n",
    "        new_df=new_df.drop(['SubInd'], axis=1)\n",
    "\n",
    "        vec_new_nlp = TfidfVectorizer(ngram_range = (1,3), max_features = 3000,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "                          stop_words = stops,  max_df=.65) \n",
    "            # ngram_range, max_features, max_df, min_df can be adjusted\n",
    "        # re can be adjusted also\n",
    "        dtm_new_nlp = vec_new_nlp.fit_transform(new_df['item1A_cleaned'])\n",
    "\n",
    "\n",
    "        dtm_tfidf_new= pd.DataFrame(dtm_new_nlp.toarray())\n",
    "        d = dict(zip(list(dtm_tfidf_new.columns), list(np.array(vec_new_nlp.get_feature_names_out()))))\n",
    "        dtm_tfidf_new.rename(columns=d, inplace=True)\n",
    "        final_merge_sec1 = pd.concat([new_df, dtm_tfidf_new], axis=1)\n",
    "        final_merge_sec1['cik']=final_merge_sec1['cik'].astype('object')\n",
    "        final_merge_sec1['actual_year']=final_merge_sec1['actual_year'].astype('object')\n",
    "\n",
    "        numeric_cols = list(final_merge_sec1.select_dtypes(include='number').columns)\n",
    "        #numeric_cols\n",
    "\n",
    "        new_df_next=final_merge_sec1.groupby(['actual_year'])[numeric_cols].sum()\n",
    "        b_next=new_df_next.transpose()\n",
    "         # This signifies that the word should be present with a \n",
    "        # higher relavance in some of the years\n",
    "        b_next=b_next[(b_next.max(axis=1) >= .25) ]\n",
    "        new_df_next=b_next.transpose()\n",
    "        new_df_next\n",
    "\n",
    "        for item_2 in list(new_df_next.columns):\n",
    "            if item_2=='index': continue\n",
    "            testing_peaks=new_df_next.loc[:,item_2]\n",
    "            \n",
    "            # This is the function that generates the spike words. \n",
    "            peaks=peaks_previousN(testing_peaks.values, 22)\n",
    "            \n",
    "            \n",
    "\n",
    "            for next_item in list(testing_peaks.iloc[peaks].index):\n",
    "                if (item not in sec1A_tfidf_model_1.keys()): sec1A_tfidf_model_1[item]=defaultdict(list)\n",
    "                sec1A_tfidf_model_1[item][next_item].append(item_2)\n",
    "\n",
    "        if item in sec1A_tfidf_model_1.keys(): sec1A_tfidf_model_1[item]=dict(sorted(sec1A_tfidf_model_1[item].items()))\n",
    "\n",
    "            #iter_dict[item].append(list(testing_peaks.iloc[peaks].index))\n",
    "        sec1A_tfidf_model_1=dict(sorted(sec1A_tfidf_model_1.items()))\n",
    "\n",
    "except:\n",
    "    print(item)\n",
    "    print(peaks)\n",
    "    #print(list(new_df_next.columns))\n",
    "    #print(list(testing_peaks.iloc[peaks]))\n",
    "    #print(list(testing_peaks.iloc[peaks].index))\n",
    "    #print(testing_peaks)\n",
    "#sec1A_tfidf_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1780ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc001375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5628279",
   "metadata": {},
   "source": [
    "### Section 1A - Model 2: \n",
    "##### The below code would create a dictionary object that has the top words / bigrams / trigrams based on Section 1A, in the SEC records belonging to the top 75 industries split by year. The top word identification is done by using a custom function that finds peaks based on how many sigmas away the peak values from the norm.\n",
    "##### Configurations that can be modified are highlighted as comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a03b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec1A_tfidf_model_2=defaultdict(defaultdict(list).copy)\n",
    "\n",
    "try:\n",
    "    \n",
    "    for item in main_75_industry:\n",
    "        if(final_merge_1A[final_merge_1A['SubInd']==item].shape[0]==0): continue\n",
    "        new_df=final_merge_1A[final_merge_1A['SubInd']==item].reset_index()\n",
    "        new_df=new_df.drop(['SubInd'], axis=1)\n",
    "\n",
    "        vec_new_nlp = TfidfVectorizer(ngram_range = (1,3), max_features = 3000,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "                          stop_words = stops,  max_df=.7) \n",
    "            # ngram_range, max_features, max_df, min_df can be adjusted\n",
    "        # re can be adjusted also\n",
    "        dtm_new_nlp = vec_new_nlp.fit_transform(new_df['item1A_cleaned'])\n",
    "\n",
    "\n",
    "        dtm_tfidf_new= pd.DataFrame(dtm_new_nlp.toarray())\n",
    "        d = dict(zip(list(dtm_tfidf_new.columns), list(np.array(vec_new_nlp.get_feature_names_out()))))\n",
    "        dtm_tfidf_new.rename(columns=d, inplace=True)\n",
    "        final_merge_sec1 = pd.concat([new_df, dtm_tfidf_new], axis=1)\n",
    "        final_merge_sec1['cik']=final_merge_sec1['cik'].astype('object')\n",
    "        final_merge_sec1['actual_year']=final_merge_sec1['actual_year'].astype('object')\n",
    "\n",
    "        numeric_cols = list(final_merge_sec1.select_dtypes(include='number').columns)\n",
    "        #numeric_cols\n",
    "\n",
    "        new_df_next=final_merge_sec1.groupby(['actual_year'])[numeric_cols].sum()\n",
    "        b_next=new_df_next.transpose()\n",
    "         # This signifies that the word should be present with a \n",
    "        # higher relavance in some of the years\n",
    "        \n",
    "        b_next=b_next[(b_next.max(axis=1) >= .25) ]\n",
    "        new_df_next=b_next.transpose()\n",
    "        new_df_next\n",
    "\n",
    "        for item_2 in list(new_df_next.columns):\n",
    "            if item_2=='index': continue\n",
    "            testing_peaks=new_df_next.loc[:,item_2]\n",
    "           # Here, we look at peaks that occur more than 1.5 Sigmas away\n",
    "            \n",
    "            peaks=np.argwhere((testing_peaks-np.mean(testing_peaks))/np.std(testing_peaks) > 1.5).ravel()\n",
    "\n",
    "            for next_item in list(testing_peaks.iloc[peaks].index):\n",
    "                if (item not in sec1A_tfidf_model_2.keys()): sec1A_tfidf_model_2[item]=defaultdict(list)\n",
    "                sec1A_tfidf_model_2[item][next_item].append(item_2)\n",
    "\n",
    "        if item in sec1A_tfidf_model_2.keys(): sec1A_tfidf_model_2[item]=\\\n",
    "            dict(sorted(sec1A_tfidf_model_2[item].items()))\n",
    "\n",
    "            #iter_dict[item].append(list(testing_peaks.iloc[peaks].index))\n",
    "        sec1A_tfidf_model_2=dict(sorted(sec1A_tfidf_model_2.items()))\n",
    "\n",
    "except:\n",
    "    print(item)\n",
    "    print(peaks)\n",
    "    #print(list(new_df_next.columns))\n",
    "    #print(list(testing_peaks.iloc[peaks]))\n",
    "    #print(list(testing_peaks.iloc[peaks].index))\n",
    "    #print(testing_peaks)\n",
    "#sec1A_tfidf_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f1081",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=final_merge_1A[final_merge_1A['SubInd']=='Interactive Media & Services'].reset_index()\n",
    "new_df=new_df.drop(['SubInd'], axis=1)\n",
    "\n",
    "vec_new_nlp = TfidfVectorizer(ngram_range = (1,3), max_features = 3000,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "                  stop_words = stops,  max_df=.7) \n",
    "dtm_new_nlp = vec_new_nlp.fit_transform(new_df['item1A_cleaned'])\n",
    "\n",
    "\n",
    "dtm_tfidf_new= pd.DataFrame(dtm_new_nlp.toarray())\n",
    "d = dict(zip(list(dtm_tfidf_new.columns), list(np.array(vec_new_nlp.get_feature_names_out()))))\n",
    "dtm_tfidf_new.rename(columns=d, inplace=True)\n",
    "final_merge_sec1 = pd.concat([new_df, dtm_tfidf_new], axis=1)\n",
    "final_merge_sec1['cik']=final_merge_sec1['cik'].astype('object')\n",
    "final_merge_sec1['actual_year']=final_merge_sec1['actual_year'].astype('object')\n",
    "\n",
    "numeric_cols = list(final_merge_sec1.select_dtypes(include='number').columns)\n",
    "#numeric_cols\n",
    "\n",
    "new_df_next=final_merge_sec1.groupby(['actual_year'])[numeric_cols].sum()\n",
    "b_next=new_df_next.transpose()\n",
    "b_next=b_next[(b_next.max(axis=1) >= .25) ]\n",
    "new_df_next=b_next.transpose()\n",
    "new_df_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b16cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_peaks=new_df_next.loc[:,'twitter']\n",
    "            #peaks=peaks_previousN(testing_peaks.values, 22)\n",
    "peaks=np.argwhere((testing_peaks-np.mean(testing_peaks))/np.std(testing_peaks) > 1.5).ravel()\n",
    "plt.plot(testing_peaks)\n",
    "plt.plot(testing_peaks.iloc[peaks].index, testing_peaks.iloc[peaks].values, \"x\")\n",
    "#plt.plot(np.zeros_like(testing_peaks), \"--\", color=\"gray\")\n",
    "plt.title(\"Spikes for covid-19 across years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678a25c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1245ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0d52dd9",
   "metadata": {},
   "source": [
    "### Sec 1A - Model 3: The below code would create a dictionary object that has the top words / bigrams / trigrams based on Section 1A, on the SEC records belonging to the top 75 industries split by year. The top word identification is done taking the top 20 words per Industry per year\n",
    "##### Configurations that can be modified are highlighted as comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec1A_tfidf_model_3=defaultdict(defaultdict(list).copy)\n",
    "\n",
    "\n",
    "    \n",
    "for item in main_75_industry:\n",
    "    if(final_merge_1A[final_merge_1A['SubInd']==item].shape[0]==0): continue\n",
    "    new_df=final_merge_1A[final_merge_1A['SubInd']==item].reset_index()\n",
    "    new_df=new_df.drop(['SubInd','index'], axis=1)\n",
    "\n",
    "    vec_new_nlp = TfidfVectorizer(ngram_range = (1,3), max_features = 3000,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "                      stop_words = stops,  max_df=.5) \n",
    "        # ngram_range, max_features, max_df, min_df can be adjusted\n",
    "        # re can be adjusted also\n",
    "    dtm_new_nlp = vec_new_nlp.fit_transform(new_df['item1A_cleaned'])\n",
    "\n",
    "\n",
    "    dtm_tfidf_new= pd.DataFrame(dtm_new_nlp.toarray())\n",
    "    d = dict(zip(list(dtm_tfidf_new.columns), list(np.array(vec_new_nlp.get_feature_names_out()))))\n",
    "    dtm_tfidf_new.rename(columns=d, inplace=True)\n",
    "\n",
    "\n",
    "    final_merge_sec1 = pd.concat([new_df, dtm_tfidf_new], axis=1)\n",
    "    final_merge_sec1['cik']=final_merge_sec1['cik'].astype('object')\n",
    "    final_merge_sec1['actual_year']=final_merge_sec1['actual_year'].astype('object')\n",
    "\n",
    "\n",
    "\n",
    "    numeric_cols = list(final_merge_sec1.select_dtypes(include='number').columns)\n",
    "    #numeric_cols\n",
    "\n",
    "    new_df_next=final_merge_sec1.groupby(['actual_year'])[numeric_cols].sum()\n",
    "    #print(new_df_next.head(5))\n",
    "\n",
    "    for item_2 in new_df_next.index:\n",
    "        temp_df=new_df_next.loc[item_2]\n",
    "        #top20 = temp_df.values.argsort()[::-1][:20]\n",
    "        #top20=sparse_argsort(temp_df)[::-1][:20]\n",
    "        top20=np.argsort(temp_df.values)[np.in1d(np.argsort(temp_df.values),np.where(temp_df.values),1)][::-1][:20]\n",
    "        #print(new_df_next.head(2) )\n",
    "        #print(item)\n",
    "        #print(top20)\n",
    "        #print(temp_df.values)\n",
    "        top_20_features=[vec_new_nlp.get_feature_names_out()[i] for i in list(top20)] \n",
    "        if (item not in sec1A_tfidf_model_3.keys()): sec1A_tfidf_model_3[item]=defaultdict(list)\n",
    "        sec1A_tfidf_model_3[item][item_2].append(top_20_features)\n",
    "\n",
    "\n",
    "\n",
    "    if item in sec1A_tfidf_model_3.keys(): sec1A_tfidf_model_3[item]=dict(sorted(sec1A_tfidf_model_3[item].items()))\n",
    "\n",
    "        #iter_dict[item].append(list(testing_peaks.iloc[peaks].index))\n",
    "    sec1A_tfidf_model_3=dict(sorted(sec1A_tfidf_model_3.items()))\n",
    "\n",
    "\n",
    "#sec1A_tfidf_model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d252ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acfd913d",
   "metadata": {},
   "source": [
    "### Sec 1A - Model 4: The below code takes the top 20 words for each of the 75 Industries across all years. Then it created seperate Word2Vec model for each year and 10 most similar words to that. This is time consuming model but the output dictionary does give back valuable inights on how words trended across years\n",
    "##### Configurations that can be modified are highlighted as comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c637c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dict_ind_top20_1A_similar=defaultdict(defaultdict(defaultdict(dict).copy).copy)\n",
    "\n",
    "\n",
    "    \n",
    "for item in main_75_industry:\n",
    "    if(final_merge_1A[final_merge_1A['SubInd']==item].shape[0]==0): continue\n",
    "    new_df=final_merge_1A[final_merge_1A['SubInd']==item].reset_index()\n",
    "    new_df=new_df.drop(['SubInd','index'], axis=1)\n",
    "\n",
    "    vec_new_nlp = TfidfVectorizer(ngram_range = (1,3), max_features = 3000,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "                      stop_words = stops,  max_df=.75) \n",
    "    \n",
    "                    # ngram_range, max_features, max_df, min_df can be adjusted\n",
    "        # re can be adjusted also\n",
    "        \n",
    "    dtm_new_nlp = vec_new_nlp.fit_transform(new_df['item1A_cleaned'])\n",
    "\n",
    "\n",
    "    dtm_tfidf_new= pd.DataFrame(dtm_new_nlp.toarray())\n",
    "    d = dict(zip(list(dtm_tfidf_new.columns), list(np.array(vec_new_nlp.get_feature_names_out()))))\n",
    "    dtm_tfidf_new.rename(columns=d, inplace=True)\n",
    "\n",
    "\n",
    "    final_merge_sec1 = pd.concat([new_df, dtm_tfidf_new], axis=1)\n",
    "    final_merge_sec1['cik']=final_merge_sec1['cik'].astype('object')\n",
    "    final_merge_sec1['actual_year']=final_merge_sec1['actual_year'].astype('object')\n",
    "\n",
    "\n",
    "\n",
    "    numeric_cols = list(final_merge_sec1.select_dtypes(include='number').columns)\n",
    "    #numeric_cols\n",
    "\n",
    "    new_df_next=final_merge_sec1.groupby(['actual_year'])[numeric_cols].sum()\n",
    "    \n",
    "    new_df_next_sum=np.sum(new_df_next, axis=0)\n",
    "    \n",
    "    top20=np.argsort(new_df_next_sum.values)[np.in1d(np.argsort(new_df_next_sum.values),\\\n",
    "                                                     np.where(new_df_next_sum.values),1)][::-1][:20]\n",
    "    top_20_features=[vec_new_nlp.get_feature_names_out()[i] for i in list(top20)] \n",
    "    \n",
    "    \n",
    "    #print(new_df_next.head(5))\n",
    "\n",
    "    for item_2 in new_df_next.index:\n",
    "        #temp_df=new_df_next.loc[item_2]\n",
    "        #top20 = temp_df.values.argsort()[::-1][:20]\n",
    "        #top20=sparse_argsort(temp_df)[::-1][:20]\n",
    "        all_sentences_inter = final_merge[final_merge['actual_year']==item_2]['item1A_cleaned'].apply(lambda x: x.split())\n",
    "        \n",
    "        bigram_transformer = Phrases(all_sentences_inter, min_count=20, delimiter=' ')\n",
    "        bigram = Phraser(bigram_transformer)\n",
    "        \n",
    "        trigram_txnformer=Phrases(bigram[all_sentences_inter], min_count=20, delimiter=' ')\n",
    "        trigram = Phraser(trigram_txnformer)\n",
    "        \n",
    "        w2v_model_bi_tri = Word2Vec(trigram[all_sentences_inter], min_count=20)\n",
    "        \n",
    "        for item3 in top_20_features:\n",
    "            if ' ' not in item3 and item3 in w2v_model_bi_tri.wv.key_to_index.keys() :\n",
    "                ea = w2v_model_bi_tri.wv.most_similar(item3)\n",
    "            elif all(x in w2v_model_bi_tri.wv.key_to_index.keys() for x in item3):\n",
    "                \n",
    "                ea = w2v_model_bi_tri.wv.most_similar(positive=item3.split())\n",
    "\n",
    "        \n",
    "            if (item not in iter_dict_ind_top20_1A_similar.keys()): \n",
    "                iter_dict_ind_top20_1A_similar[item]=defaultdict(dict)\n",
    "            if (item_2 not in  iter_dict_ind_top20_1A_similar[item].keys()):\n",
    "                iter_dict_ind_top20_1A_similar[item][item_2] = defaultdict(dict)\n",
    "            iter_dict_ind_top20_1A_similar[item][item_2][item3] = ea[:10]\n",
    "\n",
    "\n",
    "\n",
    "    if item in iter_dict_ind_top20_1A_similar.keys(): iter_dict_ind_top20_1A_similar[item]=\\\n",
    "        dict(sorted(iter_dict_ind_top20_1A_similar[item].items()))\n",
    "\n",
    "        #iter_dict[item].append(list(testing_peaks.iloc[peaks].index))\n",
    "    iter_dict_ind_top20_1A_similar=dict(sorted(iter_dict_ind_top20_1A_similar.items()))\n",
    "\n",
    "\n",
    "iter_dict_ind_top20_1A_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f148f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5993784",
   "metadata": {},
   "source": [
    "## Loading Section 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55585a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge_7 = pd.read_pickle(\"item7_cleaned.pkl\")\n",
    "final_merge_7=final_merge_7[['date','cik','company','year','item7_cleaned']]\n",
    "final_merge_7.rename(columns={'year':'actual_year', 'company': 'actual_company', 'date': 'actual_date'},inplace=True)\n",
    "final_merge_7.dropna(subset=['actual_year', 'item7_cleaned'], inplace=True)\n",
    "\n",
    "final_merge_7.reset_index(drop=True,inplace=True)\n",
    "\n",
    "final_merge_7 = final_merge_7.merge(industry_data[['ID', 'SubInd']], left_on = 'cik', right_on='ID',how = 'left')\n",
    "final_merge_7.drop(['ID'], axis=1,inplace=True)\n",
    "final_merge_7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2bfa14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8258325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_new_nlp_7 = TfidfVectorizer(ngram_range = (1,3), max_features = 2500,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "#                       stop_words = stops, max_df=.67) \n",
    "# dtm_new_nlp_7 = vec_new_nlp_7.fit_transform(final_merge_7['item7_cleaned'])\n",
    "\n",
    "# dtm_tfidf_new_7= pd.DataFrame(dtm_new_nlp_7.toarray())\n",
    "# d = dict(zip(list(dtm_tfidf_new_7.columns), list(np.array(vec_new_nlp_7.get_feature_names_out()))))\n",
    "# dtm_tfidf_new_7.rename(columns=d, inplace=True)\n",
    "# final_merge_sec_7 = pd.concat([final_merge_7, dtm_tfidf_new_7], axis=1)\n",
    "# final_merge_sec_7['cik']=final_merge_sec_7['cik'].astype('object')\n",
    "# final_merge_sec_7['actual_year']=final_merge_sec_7['actual_year'].astype('object')\n",
    "\n",
    "# numeric_cols = list(final_merge_sec_7.select_dtypes(include='number').columns)\n",
    "# final_merge_transpose_7 = final_merge_sec_7.groupby(['actual_year'])[numeric_cols].sum()\n",
    "\n",
    "# final_merge_groupby_subind_7 = final_merge_sec_7.groupby(['SubInd','actual_year'])[numeric_cols].sum().reset_index()\n",
    "# final_merge_groupby_subind_7.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d502a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c948e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91e455bd",
   "metadata": {},
   "source": [
    "### Sec 7 - Model 1: \n",
    "##### The below code would create a dictionary object that has the top words / bigrams / trigrams based on Section 7, in the SEC records belonging to the top 75 industries split by year. The top word identification is done by using a custom function that finds peaks in data based on its tfidf distribution\n",
    "##### Configurations that can be modified are highlighted as comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec7_tfidf_model_1=defaultdict(defaultdict(list).copy)\n",
    "\n",
    "try:\n",
    "    \n",
    "    for item in main_75_industry:\n",
    "        if(final_merge_7[final_merge_7['SubInd']==item].shape[0]==0): continue\n",
    "        new_df=final_merge_7[final_merge_7['SubInd']==item].reset_index()\n",
    "        new_df=new_df.drop(['SubInd'], axis=1)\n",
    "\n",
    "        vec_new_nlp = TfidfVectorizer(ngram_range = (1,3), max_features = 3000,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "                          stop_words = stops,  max_df=.6) \n",
    "        \n",
    "                # ngram_range, max_features, max_df, min_df can be adjusted\n",
    "        # re can be adjusted also\n",
    "        \n",
    "        dtm_new_nlp = vec_new_nlp.fit_transform(new_df['item7_cleaned'])\n",
    "\n",
    "\n",
    "        dtm_tfidf_new= pd.DataFrame(dtm_new_nlp.toarray())\n",
    "        d = dict(zip(list(dtm_tfidf_new.columns), list(np.array(vec_new_nlp.get_feature_names_out()))))\n",
    "        dtm_tfidf_new.rename(columns=d, inplace=True)\n",
    "        final_merge_sec1 = pd.concat([new_df, dtm_tfidf_new], axis=1)\n",
    "        final_merge_sec1['cik']=final_merge_sec1['cik'].astype('object')\n",
    "        final_merge_sec1['actual_year']=final_merge_sec1['actual_year'].astype('object')\n",
    "\n",
    "        numeric_cols = list(final_merge_sec1.select_dtypes(include='number').columns)\n",
    "        #numeric_cols\n",
    "\n",
    "        new_df_next=final_merge_sec1.groupby(['actual_year'])[numeric_cols].sum()\n",
    "        b_next=new_df_next.transpose()\n",
    "        \n",
    "        # This signifies that the word should be present with a \n",
    "        # higher relavance in some of the years\n",
    "        \n",
    "        \n",
    "        b_next=b_next[(b_next.max(axis=1) >= .2) ]\n",
    "        new_df_next=b_next.transpose()\n",
    "        new_df_next\n",
    "\n",
    "        for item_2 in list(new_df_next.columns):\n",
    "            if item_2=='index': continue\n",
    "            testing_peaks=new_df_next.loc[:,item_2]\n",
    "            \n",
    "            # This is the function that generates the spike words.\n",
    "            \n",
    "            peaks=peaks_previousN(testing_peaks.values, 22)\n",
    "\n",
    "            for next_item in list(testing_peaks.iloc[peaks].index):\n",
    "                if (item not in sec7_tfidf_model_1.keys()): sec7_tfidf_model_1[item]=defaultdict(list)\n",
    "                sec7_tfidf_model_1[item][next_item].append(item_2)\n",
    "\n",
    "        if item in sec7_tfidf_model_1.keys(): sec7_tfidf_model_1[item]=dict(sorted(sec7_tfidf_model_1[item].items()))\n",
    "\n",
    "            #iter_dict[item].append(list(testing_peaks.iloc[peaks].index))\n",
    "        sec7_tfidf_model_1=dict(sorted(sec7_tfidf_model_1.items()))\n",
    "\n",
    "except:\n",
    "    print(item)\n",
    "    print(peaks)\n",
    "    #print(list(new_df_next.columns))\n",
    "    #print(list(testing_peaks.iloc[peaks]))\n",
    "    #print(list(testing_peaks.iloc[peaks].index))\n",
    "    #print(testing_peaks)\n",
    "#sec7_tfidf_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7abfb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78e0c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf2070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fad275b",
   "metadata": {},
   "source": [
    "### Section 7 - Model 2: \n",
    "##### The below code would create a dictionary object that has the top words / bigrams / trigrams based on Section 7, in the SEC records belonging to the top 75 industries split by year. The top word identification is done by using a custom function that finds peaks based on how many sigmas away the peak values from the norm.\n",
    "##### Configurations that can be modified are highlighted as comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8cda9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec7_tfidf_model_2=defaultdict(defaultdict(list).copy)\n",
    "sec7_industry_dict=defaultdict(list)\n",
    "\n",
    "try:\n",
    "    \n",
    "    for item in main_75_industry:\n",
    "        if(final_merge_7[final_merge_7['SubInd']==item].shape[0]==0): continue\n",
    "        new_df=final_merge_7[final_merge_7['SubInd']==item].reset_index()\n",
    "        new_df=new_df.drop(['SubInd'], axis=1)\n",
    "\n",
    "        vec_new_nlp = TfidfVectorizer(ngram_range = (1,3), max_features = 3000,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "                          stop_words = stops,  max_df=.7) \n",
    "        \n",
    "                # ngram_range, max_features, max_df, min_df can be adjusted\n",
    "        # re can be adjusted also\n",
    "        \n",
    "        dtm_new_nlp = vec_new_nlp.fit_transform(new_df['item7_cleaned'])\n",
    "\n",
    "\n",
    "        dtm_tfidf_new= pd.DataFrame(dtm_new_nlp.toarray())\n",
    "        d = dict(zip(list(dtm_tfidf_new.columns), list(np.array(vec_new_nlp.get_feature_names_out()))))\n",
    "        dtm_tfidf_new.rename(columns=d, inplace=True)\n",
    "        final_merge_sec1 = pd.concat([new_df, dtm_tfidf_new], axis=1)\n",
    "        final_merge_sec1['cik']=final_merge_sec1['cik'].astype('object')\n",
    "        final_merge_sec1['actual_year']=final_merge_sec1['actual_year'].astype('object')\n",
    "\n",
    "        numeric_cols = list(final_merge_sec1.select_dtypes(include='number').columns)\n",
    "        #numeric_cols\n",
    "\n",
    "        new_df_next=final_merge_sec1.groupby(['actual_year'])[numeric_cols].sum()\n",
    "        b_next=new_df_next.transpose()\n",
    "        \n",
    "        # This signifies that the word should be present with a \n",
    "        # higher relavance in some of the years\n",
    "        \n",
    "        \n",
    "        b_next=b_next[(b_next.max(axis=1) >= .2) ]\n",
    "        new_df_next=b_next.transpose()\n",
    "        new_df_next\n",
    "        a_list=[]\n",
    "\n",
    "        for item_2 in list(new_df_next.columns):\n",
    "            if item_2=='index': continue\n",
    "            testing_peaks=new_df_next.loc[:,item_2]\n",
    "            \n",
    "            \n",
    "                      # Here, we look at peaks that occur more than 2.5 Sigmas away\n",
    "            \n",
    "            peaks=np.argwhere((testing_peaks-np.mean(testing_peaks))/np.std(testing_peaks) > 2.5).ravel()\n",
    "\n",
    "            for next_item in list(testing_peaks.iloc[peaks].index):\n",
    "                if (item not in sec7_tfidf_model_2.keys()): sec7_tfidf_model_2[item]=defaultdict(list)\n",
    "                sec7_tfidf_model_2[item][next_item].append(item_2)\n",
    "                if next_item > 2019: sec7_industry_dict[item].append(item_2)\n",
    "\n",
    "        sec7_industry_dict[item]  = a_list[:20]\n",
    "        a_list=[]\n",
    "        if item in sec7_tfidf_model_2.keys(): sec7_tfidf_model_2[item]=\\\n",
    "            dict(sorted(sec7_tfidf_model_2[item].items()))\n",
    "\n",
    "            #iter_dict[item].append(list(testing_peaks.iloc[peaks].index))\n",
    "        sec7_tfidf_model_2=dict(sorted(sec7_tfidf_model_2.items()))\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(peaks)\n",
    "    #print(list(new_df_next.columns))\n",
    "    #print(list(testing_peaks.iloc[peaks]))\n",
    "    #print(list(testing_peaks.iloc[peaks].index))\n",
    "    #print(testing_peaks)\n",
    "#sec7_tfidf_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52204c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2014432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f17a79f1",
   "metadata": {},
   "source": [
    "### Sec 7 - Model 3: The below code would create a dictionary object that has the top words / bigrams / trigrams based on Section 7, in the SEC records belonging to the top 75 industries split by year. The top word identification is done taking the top 20 words per Industry per year\n",
    "##### Configurations that can be modified are highlighted as comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4084f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39964b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec7_tfidf_model_3=defaultdict(defaultdict(list).copy)\n",
    "\n",
    "\n",
    "    \n",
    "for item in main_75_industry:\n",
    "    if(final_merge_7[final_merge_7['SubInd']==item].shape[0]==0): continue\n",
    "    new_df=final_merge_7[final_merge_7['SubInd']==item].reset_index()\n",
    "    new_df=new_df.drop(['SubInd','index'], axis=1)\n",
    "\n",
    "    vec_new_nlp = TfidfVectorizer(ngram_range = (1,1), max_features = 3000,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "                      stop_words = stops,  max_df=.7) \n",
    "    \n",
    "            # ngram_range, max_features, max_df, min_df can be adjusted\n",
    "        # re can be adjusted also\n",
    "        \n",
    "    dtm_new_nlp = vec_new_nlp.fit_transform(new_df['item7_cleaned'])\n",
    "\n",
    "\n",
    "    dtm_tfidf_new= pd.DataFrame(dtm_new_nlp.toarray())\n",
    "    d = dict(zip(list(dtm_tfidf_new.columns), list(np.array(vec_new_nlp.get_feature_names_out()))))\n",
    "    dtm_tfidf_new.rename(columns=d, inplace=True)\n",
    "\n",
    "\n",
    "    final_merge_sec1 = pd.concat([new_df, dtm_tfidf_new], axis=1)\n",
    "    final_merge_sec1['cik']=final_merge_sec1['cik'].astype('object')\n",
    "    final_merge_sec1['actual_year']=final_merge_sec1['actual_year'].astype('object')\n",
    "\n",
    "\n",
    "\n",
    "    numeric_cols = list(final_merge_sec1.select_dtypes(include='number').columns)\n",
    "    \n",
    "\n",
    "    new_df_next=final_merge_sec1.groupby(['actual_year'])[numeric_cols].sum()\n",
    "   \n",
    "\n",
    "    for item_2 in new_df_next.index:\n",
    "        temp_df=new_df_next.loc[item_2]\n",
    "        \n",
    "        top20=np.argsort(temp_df.values)[np.in1d(np.argsort(temp_df.values),np.where(temp_df.values),1)][::-1][:20]\n",
    "       \n",
    "        top_20_features=[vec_new_nlp.get_feature_names_out()[i] for i in list(top20)] \n",
    "        if (item not in sec7_tfidf_model_3.keys()): sec7_tfidf_model_3[item]=defaultdict(list)\n",
    "        sec7_tfidf_model_3[item][item_2].append(top_20_features)\n",
    "\n",
    "\n",
    "\n",
    "    if item in sec7_tfidf_model_3.keys(): sec7_tfidf_model_3[item]=dict(sorted(sec7_tfidf_model_3[item].items()))\n",
    "\n",
    "        \n",
    "    sec7_tfidf_model_3=dict(sorted(sec7_tfidf_model_3.items()))\n",
    "\n",
    "\n",
    "#sec7_tfidf_model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a52ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536edd38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9e4fd80",
   "metadata": {},
   "source": [
    "### Sec 7 - Model 4: The below code takes the top 20 words for each of the 75 Industries across all years. Then it created seperate Word2Vec model for each year and 10 most similar words to that. This is time consuming model but the output dictionary does give back valuable inights on how words trended across years\n",
    "##### Configurations that can be modified are highlighted as comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705fbebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f5df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dict_ind_top20_7_similar=defaultdict(defaultdict(defaultdict(dict).copy).copy)\n",
    "\n",
    "\n",
    "    \n",
    "for item in main_75_industry:\n",
    "    if(final_merge_7[final_merge_7['SubInd']==item].shape[0]==0): continue\n",
    "    new_df=final_merge_7[final_merge_7['SubInd']==item].reset_index()\n",
    "    new_df=new_df.drop(['SubInd','index'], axis=1)\n",
    "\n",
    "    vec_new_nlp = TfidfVectorizer(ngram_range = (1,3), max_features = 3000,token_pattern=r'[0-9]*[a-zA-Z]+[a-zA-Z0-9-_]+', \\\n",
    "                      stop_words = stops,  max_df=.75) \n",
    "    \n",
    "                    # ngram_range, max_features, max_df, min_df can be adjusted\n",
    "        # re can be adjusted also\n",
    "    dtm_new_nlp = vec_new_nlp.fit_transform(new_df['item7_cleaned'])\n",
    "\n",
    "\n",
    "    dtm_tfidf_new= pd.DataFrame(dtm_new_nlp.toarray())\n",
    "    d = dict(zip(list(dtm_tfidf_new.columns), list(np.array(vec_new_nlp.get_feature_names_out()))))\n",
    "    dtm_tfidf_new.rename(columns=d, inplace=True)\n",
    "\n",
    "\n",
    "    final_merge_sec1 = pd.concat([new_df, dtm_tfidf_new], axis=1)\n",
    "    final_merge_sec1['cik']=final_merge_sec1['cik'].astype('object')\n",
    "    final_merge_sec1['actual_year']=final_merge_sec1['actual_year'].astype('object')\n",
    "\n",
    "\n",
    "\n",
    "    numeric_cols = list(final_merge_sec1.select_dtypes(include='number').columns)\n",
    "    #numeric_cols\n",
    "\n",
    "    new_df_next=final_merge_sec1.groupby(['actual_year'])[numeric_cols].sum()\n",
    "    \n",
    "    new_df_next_sum=np.sum(new_df_next, axis=0)\n",
    "    \n",
    "    top20=np.argsort(new_df_next_sum.values)[np.in1d(np.argsort(new_df_next_sum.values),\\\n",
    "                                                     np.where(new_df_next_sum.values),1)][::-1][:20]\n",
    "    top_20_features=[vec_new_nlp.get_feature_names_out()[i] for i in list(top20)] \n",
    "    \n",
    "    \n",
    "    #print(new_df_next.head(5))\n",
    "\n",
    "    for item_2 in new_df_next.index:\n",
    "        #temp_df=new_df_next.loc[item_2]\n",
    "        #top20 = temp_df.values.argsort()[::-1][:20]\n",
    "        #top20=sparse_argsort(temp_df)[::-1][:20]\n",
    "        all_sentences_inter = final_merge[final_merge['actual_year']==item_2]['item7_cleaned'].apply(lambda x: x.split())\n",
    "        \n",
    "        bigram_transformer = Phrases(all_sentences_inter, min_count=20, delimiter=' ')\n",
    "        bigram = Phraser(bigram_transformer)\n",
    "        \n",
    "        trigram_txnformer=Phrases(bigram[all_sentences_inter], min_count=20, delimiter=' ')\n",
    "        trigram = Phraser(trigram_txnformer)\n",
    "        \n",
    "        w2v_model_bi_tri = Word2Vec(trigram[all_sentences_inter], min_count=20)\n",
    "        \n",
    "        for item3 in top_20_features:\n",
    "            if ' ' not in item3 and item3 in w2v_model_bi_tri.wv.key_to_index.keys() :\n",
    "                ea = w2v_model_bi_tri.wv.most_similar(item3)\n",
    "            elif all(x in w2v_model_bi_tri.wv.key_to_index.keys() for x in item3):\n",
    "                \n",
    "                ea = w2v_model_bi_tri.wv.most_similar(positive=item3.split())\n",
    "\n",
    "        \n",
    "            if (item not in iter_dict_ind_top20_7_similar.keys()): \n",
    "                iter_dict_ind_top20_7_similar[item]=defaultdict(dict)\n",
    "            if (item_2 not in  iter_dict_ind_top20_7_similar[item].keys()):\n",
    "                iter_dict_ind_top20_7_similar[item][item_2] = defaultdict(dict)\n",
    "            iter_dict_ind_top20_7_similar[item][item_2][item3] = ea[:10]\n",
    "\n",
    "\n",
    "\n",
    "    if item in iter_dict_ind_top20_7_similar.keys(): iter_dict_ind_top20_7_similar[item]=\\\n",
    "        dict(sorted(iter_dict_ind_top20_7_similar[item].items()))\n",
    "\n",
    "        #iter_dict[item].append(list(testing_peaks.iloc[peaks].index))\n",
    "    iter_dict_ind_top20_7_similar=dict(sorted(iter_dict_ind_top20_7_similar.items()))\n",
    "\n",
    "\n",
    "#iter_dict_ind_top20_7_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f2eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383fdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11698b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(r'item1_similar.json')\n",
    " \n",
    "# # returns JSON object as \n",
    "# # a dictionary\n",
    "# data = json.load(f)\n",
    " \n",
    "# # Iterating through the json\n",
    "# # list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072df4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', None)\n",
    "# inter=pd.concat({k: pd.DataFrame.from_dict(v, orient='index') \\\n",
    "#                  for k, v in dict(list(data.items())).items() if k=='Biotechnology' }\\\n",
    "#          )['vaccine'].reset_index()\n",
    "# inter.drop(columns=['level_0'], inplace=True)\n",
    "# inter.index=inter['level_1']\n",
    "# inter.index.names = ['year']\n",
    "# inter.drop(columns=['level_1'], inplace=True)\n",
    "# inter['vaccine']=inter['vaccine'].apply(lambda x: [item[0] for item in x])\n",
    "# inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb45d350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ba56cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acf161f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c1ee276",
   "metadata": {},
   "source": [
    "## Global Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad65f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "w2v_google = gensim.downloader.load('word2vec-google-news-300') # this will take a few minutes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e16d74",
   "metadata": {},
   "source": [
    "#### We are creating a single corpus combining data from Section 1, 1A and 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge_1['list_item1_cleaned']=final_merge_1['item1_cleaned'].apply(lambda x: x.split())\n",
    "final_merge_1A['list_item1A_cleaned']=final_merge_1A['item1A_cleaned'].apply(lambda x: x.split())\n",
    "final_merge_7['list_item7_cleaned']=final_merge_7['item7_cleaned'].apply(lambda x: x.split())\n",
    "all_sentences = pd.concat([final_merge_1['list_item1_cleaned'], final_merge_1A['list_item1A_cleaned']\\\n",
    "                          ,final_merge_7['list_item7_cleaned']], ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e79cc",
   "metadata": {},
   "source": [
    "#### Some points:\n",
    "#### We need to create bigrams and trigrams. One aspect of that is that we need to put in a threshold for scoring in terms of in how many documents should the phrase appear (this is the min_count). Details present in  https://radimrehurek.com/gensim/models/phrases.html) can explain some of these aspects around scoring and threshold.\n",
    "#### plus what should be the threshold. So all phrases above and equal to the threshold are extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab480d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_sentences.dropna(inplace=True)\n",
    "bigram_transformer = Phrases(all_sentences, min_count=100, delimiter=' ', scoring='npmi', threshold=.65, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "bigram = Phraser(bigram_transformer)\n",
    "trigram_txnformer=Phrases(bigram_transformer[all_sentences], min_count=100, delimiter=' ', scoring='npmi',  threshold=.65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6045c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = Phraser(trigram_txnformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_bi_tri = Word2Vec(trigram[all_sentences], min_count=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf8f836",
   "metadata": {},
   "source": [
    "## Finally, we can take some of the words which we extracted from earlier Spike tests and use it identify closley matching words in the google model, and our global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f79ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "goog=''\n",
    "for term in ['ai','management', 'layoff', 'covid-19', 'bitcoin','5g','cannabis','marijuana']:\n",
    "   \n",
    "    if ' ' not in term:\n",
    "        \n",
    "        w2v_vocab = w2v_google.key_to_index.keys()\n",
    "        if term in w2v_vocab: # Check if word in vocab\n",
    "            goog = w2v_google.most_similar(term)\n",
    "\n",
    "        ea = w2v_model_bi_tri.wv.most_similar(term) # Word2Vec requires the extra \"wv\"\n",
    "        #mod =  w2v_ea.wv.most_similar(term) # Word2Vec requires the extra \"wv\"\n",
    "        if goog: print(f\"The five terms most similar to {term} using the google based model are {', '.join([s[0] for s in goog[:5]])}.\")\n",
    "        print(f\"The five terms most similar to {term} using the custom model are {', '.join([s[0] for s in ea[:5]])}.\")\n",
    "        #print(f\"The five terms most similar to {term} using the bigram model are {', '.join([s[0] for s in mod[:5]])}.\")\n",
    "        print(\"\\n\")\n",
    "        goog=''\n",
    "    else:\n",
    "        #w2v_vocab = w2v_google.key_to_index.keys()\n",
    "       # if term in w2v_vocab: # Check if word in vocab\n",
    "           # goog = w2v_google.most_similar(term)\n",
    "\n",
    "        ea = w2v_ea.wv.most_similar(positive=term.split()) # Word2Vec requires the extra \"wv\"\n",
    "        #mod = w2v_ea.wv.most_similar(positive=term.split()) # Word2Vec requires the extra \"wv\"\n",
    "        if goog: print(f\"The five terms most similar to {term} using the google based model are {', '.join([s[0] for s in goog[:5]])}.\")\n",
    "        print(f\"The five terms most similar to {term} using the custom model are {', '.join([s[0] for s in ea[:5]])}.\")\n",
    "        #print(f\"The five terms most similar to {term} using the bigram model are {', '.join([s[0] for s in mod[:5]])}.\")\n",
    "        print(\"\\n\")\n",
    "       # goog=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a91ea7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
